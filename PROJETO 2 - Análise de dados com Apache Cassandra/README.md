# üöÄ ***Apache Cassandra para processamento de an√°lise de dados***
  

## üìñ **Descri√ß√£o do Projeto:**
O projeto utiliza Apache Cassandra para processar e analisar dados de usu√°rios e sess√µes de m√∫sica. O pipeline extrai, transforma e carrega dados em um cluster Cassandra, permitindo a execu√ß√£o de consultas anal√≠ticas.


## Principais Funcionalidades:
- Extra√ß√£o e transforma√ß√£o de dados de m√∫ltiplos arquivos CSV.
- Cria√ß√£o de tabelas no Cassandra e carga de dados para an√°lises espec√≠ficas.
- Execu√ß√£o de tr√™s pipelines de Analytics para responder perguntas de neg√≥cios.
- Gera√ß√£o de relat√≥rios em CSV com os resultados das consultas.


## üõ†Ô∏è Ferramentas Utilizadas:
- **Apache Cassandra**: Banco de dados NoSQL para armazenamento e an√°lise dos dados.
- **Python**: Para manipula√ß√£o dos dados e execu√ß√£o dos pipelines.
- **Pandas**: Para an√°lise e manipula√ß√£o dos dados no formato DataFrame.
- **CQL**: Linguagem de consulta Cassandra para manipula√ß√£o de dados e keyspaces.



## üìã **Descri√ß√£o do Processo:**
. **Instala√ß√£o do Apache Cassandra**:
   - Configura√ß√£o de uma VM Linux.
   - Cria√ß√£o de reposit√≥rio e instala√ß√£o via `yum`.
   - Inicializa√ß√£o e configura√ß√£o do Cassandra para iniciar automaticamente.

2. **Instala√ß√£o do Python**:
   - Instala√ß√£o do Anaconda Python e do driver do Cassandra com `pip`.

3. **ETL**:
   - **Extra√ß√£o**: Consolida√ß√£o de 30 arquivos CSV em um √∫nico arquivo.
   - **Transforma√ß√£o**: Filtragem dos dados relevantes para inser√ß√£o no Cassandra.
   - **Carga e Analytics**: Inser√ß√£o dos dados transformados em tabelas do Cassandra e execu√ß√£o de consultas para an√°lises de neg√≥cios.



## üíª **Comandos:** 

### Instala√ß√£o do Apache Cassandra para RHEL 7

#### Criar uma VM com ambiente linux 

#### Editar e criar o reposit√≥rio: 

/etc/yum.repos.d/cassandra.repo

```
[cassandra]
name=Apache Cassandra
baseurl=https://redhat.cassandra.apache.org/41x/noboolean/
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://downloads.apache.org/cassandra/KEYS
```

#### Instale o Cassandra, aceitando os prompts de importa√ß√£o da chave gpg:

sudo yum install cassandra

#### Inicie o Cassandra (n√£o ser√° iniciado automaticamente):

service cassandra start

#### Fa√ßa o Cassandra iniciar automaticamente ap√≥s a 
reinicializa√ß√£o:

chkconfig cassandra on

#### Conferir o status

sudo systemctl status cassandra

---

### Instalar Python 

Fazer o download do arquivo Anaconda Pyhton > No terminal digitar: bash <Nome do arquivo de download> > navegar at√© cd ~ > source .bashrc (para atualizar as informa√ß√µes do Anaconda)

---

### Instalar o driver Cassandra

pip install cassandra-driver 

---

### Dados de amostra utilizados 

#datasets (S√£o 30 arquivos)

artist,auth,firstName,gender,itemInSession,lastName,length,level,location,method,page,registration,sessionId,song,status,ts,userId

Mudhoney,Logged In,Aleena,F,10,Kirby,231.57506,paid,"Waterloo-Cedar Falls, IA",PUT,NextSong,1.54102E+12,637,Get Into Yours,200,1.54233E+12,44

Carpenters,Logged In,Aleena,F,11,Kirby,238.39302,paid,"Waterloo-Cedar Falls, IA",PUT,NextSong,1.54102E+12,637,Yesterday Once More,200,1.54233E+12,44

---

### Script ETL para Extra√ß√£o e Transforma√ß√£o (Carga e Analytics est√° no outro script)

#etl_app.py

```py
# Imports
import os
import glob
import csv
from pipeline import conecta_cluster

# Fun√ß√£o para consolidar os arquivos de entrada em um √∫nico arquivo
def etl_processa_arquivos():

    print("\nIniciando a Etapa 1 do ETL...")

    # Pasta com os arquivos que ser√£o processados
    current_path = "dados"
    print("\nOs arquivos que ser√£o processados est√£o na pasta: " + current_path)

    # Lista para o caminho de cada arquivo
    lista_caminho_arquivos = []

    # Loop para extrair o caminho de cada arquivo
    print("\nExtraindo o caminho de cada arquivo.")
    for root, dirs, files in os.walk(current_path):
        lista_caminho_arquivos = glob.glob(os.path.join(root, '*'))

    # Lista para manipular as linhas de cada arquivo
    linhas_dados_all = list()

    # Loop por cada arquivo
    print("\nExtraindo as linhas de cada arquivo e consolidando em um novo arquivo.")
    for file in lista_caminho_arquivos:
        with open(file, 'r', encoding = 'utf8', newline = '') as fh:
            reader = csv.reader(fh)
            next(reader)

            # Append de cada linha de cada arquivo
            for line in reader:
                linhas_dados_all.append(line)

    print("\nEtapa 1 Finalizada. Extra√ß√£o conclu√≠da com sucesso.")

    return linhas_dados_all
```

---

### Fun√ß√£o para extrair somente os dados relevantes do arquivo gerado pela fun√ß√£o anterior

```py
def etl_processa_dados(records):

    print("\nIniciando a Etapa 2 do ETL...")

    # Registra uma estrutura de dados
    csv.register_dialect('dadosGerais', quoting = csv.QUOTE_ALL, skipinitialspace = True)

    # Filtrando os dados relevantes que ser√£o inseridos no Apache Cassandra
    print("\nFiltrando os dados relevantes que ser√£o inseridos no Apache Cassandra.")
    with open('resultado/dataset_completo.csv', 'w', encoding = 'utf8', newline = '') as fh:

        # Cria o objeto
        writer = csv.writer(fh)

        # Executa
        writer.writerow(['artist', 'firstName', 'gender', 'itemInSession', 'lastName', 'length','level', 'location', 'sessionId', 'song', 'userId'])

        # Loop
        for row in records:
            # Se n√£o tiver artista, n√£o geramos a linha final
            if not row[0]:
                continue
            
            # Gravo a linha de dados
            writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))

        print("\nEtapa 2 Finalizada. Transforma√ß√£o conclu√≠da com sucesso.")

# Bloco main
if __name__ == '__main__':
    
    # Etapa 1 do ETL
    records_list = etl_processa_arquivos()

    # Se a Etapa 1 foi executada com sucesso, seguimos para a pr√≥xima etapa
    if records_list:

        # Etapa 2 do ETL 
        etl_processa_dados(records_list)

        # Conectamos no cluster para a carga de dados no Cassandra
        conecta_cluster()
```


---

### Carga de dados e Analytics
#pipeline.py

```py
# Imports
import csv
import pandas as pd
from cassandra.cluster import Cluster

# Este arquivo n√£o deve ser executado diretamente. Colocamos uma mensagem para lembrar o Engenheiro de Dados.
if __name__ == '__main__':
    print("Este arquivo n√£o deve ser executado diretamente. Execute: etl_app.py")

# Arquivo de dados que ser√° carregado no Apache Cassandra
file_name = 'resultado/dataset_completo.csv'

# Fun√ß√£o para criar o cluster Cassandra
def conecta_cluster():

    # Cria a conex√£o ao cluster Cassandra
    cluster = Cluster(['localhost'])

    # Estabelece a conex√£o
    session = cluster.connect()

    # Cria a keyspace
    session.execute("CREATE KEYSPACE IF NOT EXISTS projeto1 WITH REPLICATION = "
                    "{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 }")

    # Define o tipo da keyspace
    session.set_keyspace('projeto1')

    # Executa os m√©todos de Analytics
    pipeline_analytics_1(session)
    pipeline_analytics_2(session)
    pipeline_analytics_3(session)

    # Deleta as tabelas ap√≥s o Analytics
    # drop_tables(session)

    # Desliga os recursos
    session.shutdown()
    cluster.shutdown()

    print("\nPipeline Conclu√≠do com Sucesso. Obrigado!\n")

# Pipeline de Analytics 1 - Busca o artista e o comprimento (tempo) da m√∫sica do sessionId = 436 e itemInSession = 12
def pipeline_analytics_1(session):

    print("\nIniciando o Pipeline de Analytics 1 (Carga e An√°lise de Dados)...")

    # Cria a tabela
    query = "CREATE TABLE IF NOT EXISTS tb_session_itemSession "
    
    # Cria as colunas na tabela
    query = query + "(sessionId text, itemInSession text, song text, artist text, length text, " \
                    "PRIMARY KEY (sessionId, itemInSession))"
    
    # Executa
    session.execute(query)

    # Abre o arquivo de entrada para a carga de dados
    with open(file_name, 'r', encoding = 'utf-8') as fh:

        # Leitura do arquivo
        reader = csv.reader(fh)
        next(reader)

        # Loop pelo arquivo e carga na tebal no Cassandra
        for line in reader:
            query = "INSERT INTO tb_session_itemSession (sessionId, itemInSession, song, artist, length)"
            query = query + " VALUES (%s, %s, %s, %s, %s)"
            session.execute(query, (line[8], line[3], line[9], line[0], line[5]))

    # Select nos dados
    query = "SELECT artist, song, length FROM tb_session_itemSession WHERE sessionId = '436' and itemInSession = '12'"
    
    # Converte o resultado em dataframe do Pandas e salva em disco
    df = pd.DataFrame(list(session.execute(query)))
    df.to_csv('resultado/pipeline1.csv', sep = ',', encoding = 'utf-8')
    
    # Print
    print("\nResultado do Pipeline de Analytics 1:\n")
    print(df)

# Pipeline de Analytics 2 - Busca o artista, o nome da m√∫sica e o usu√°rio do userid = 54 e sessionid = 616
def pipeline_analytics_2(session):

    print("\nIniciando o Pipeline de Analytics 2 (Carga e An√°lise de Dados)...")

    # Cria a tabela
    query = "CREATE TABLE IF NOT EXISTS tb_user_session "
    query = query + "(userId text, sessionId text, itemInSession text, artist text, song text, firstName text, " \
                    "lastName text, PRIMARY KEY ((userId, sessionId), itemInSession))"
    session.execute(query)

    # Carrega a tabela
    with open(file_name, 'r', encoding = 'utf8') as f:
        reader = csv.reader(f)
        next(reader) 
        for line in reader:
            query = "INSERT INTO tb_user_session (userId, sessionId, itemInSession, artist, song, firstName, lastName)"
            query = query + " VALUES (%s, %s, %s, %s, %s, %s, %s)"
            session.execute(query, (line[10], line[8], line[3], line[0], line[9], line[1], line[4]))

    # Analisa a tabela
    query = "SELECT artist, song, firstname, lastname FROM tb_user_session WHERE userId = '54' and sessionId = '616'"

    # Resultado
    df = pd.DataFrame(list(session.execute(query)))
    df.to_csv('resultado/pipeline2.csv', sep = ',', encoding = 'utf-8')
    print("\nResultado do Pipeline de Analytics 2:\n")
    print(df)

# Pipeline de Analytics 3 - Busca cada usu√°rio que ouviu a m√∫sica 'The Rhythm Of The Night'
def pipeline_analytics_3(session):

    print("\nIniciando o Pipeline de Analytics 3 (Carga e An√°lise de Dados)...")

    # Cria a tabela
    query = "CREATE TABLE IF NOT EXISTS tb_user_song "
    query = query + "(song text, userId text, firstName text, lastName text, PRIMARY KEY (song, userId))"
    session.execute(query)

    # Carrega a tabela
    with open(file_name, 'r', encoding = 'utf8') as f:
        reader = csv.reader(f)
        next(reader) 
        for line in reader:
            query = "INSERT INTO tb_user_song (song, userId, firstName, lastName)"
            query = query + " VALUES (%s, %s, %s, %s)"
            session.execute(query, (line[9], line[10], line[1], line[4]))

    # Analisa a tabela
    query = "SELECT firstname, lastname FROM tb_user_song WHERE song = 'The Rhythm Of The Night'"
    
    # Resultado
    df = pd.DataFrame(list(session.execute(query)))
    df.to_csv('resultado/pipeline3.csv', sep = ',', encoding = 'utf-8')
    print("\nResultado do Pipeline de Analytics 3:\n")
    print(df)

# Fun√ß√£o para deletar as tabelas ao final do pipeline
def drop_tables(session):

    query = "DROP TABLE tb_session_itemSession"
    session.execute(query)

    query = "DROP TABLE tb_user_session"
    session.execute(query)

    query = "DROP TABLE tb_user_song"
    session.execute(query)

``` 
---

### Perguntas respondidas com esse pipeline:

#### Qual o artista e o comprimento (tempo) da m√∫sica do sessionId = 436 e itemInSession = 12?

#### Quais m√∫sicas o usu√°rio do userid = 54 e sessionid = 616 ouviu? Retorne o nome do artista, o nome da m√∫sica e nome e sobrenome do usu√°rio.

#### Quais usu√°rios ouviram a m√∫sica 'The Rhythm Of The Night'?

#Executando o pipeline:
1. Crie a pasta Projeto1 e coloque l√° dentro todos os arquivos do projeto.

2. No terminal, acesse a pasta e execute o pipeline: sudo python etl_app.py

---

### Trabalhando com CQL

#### Instru√ß√µes CQL

```sql
DESCRIBE keyspaces;

SELECT * FROM system_schema.keyspaces;

DESCRIBE tables;

DESCRIBE projeto1;

DESCRIBE projeto1.tb_user_session;

SELECT sessionid, COUNT(*) FROM projeto1.tb_user_session WHERE userid IN ('49', '50') GROUP BY sessionid;  

SELECT sessionid, COUNT(*) FROM projeto1.tb_user_session WHERE userid IN ('49', '50') GROUP BY sessionid ALLOW FILTERING;

SELECT sessionid, COUNT(*) FROM projeto1.tb_user_session WHERE userid = '49' GROUP BY sessionid ALLOW FILTERING;
```

#### Sair do CQLSH e digitar os comandos abaixo para verificar a sa√≠de do cluster

nodetool tablestats

nodetool describecluster



OBS: Realizado na vm datanode


---
## üìû **Contato**

Se tiver d√∫vidas ou sugest√µes sobre o projeto, entre em contato comigo:

- üíº [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- üê± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)